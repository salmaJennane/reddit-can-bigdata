"""
========================================================
AIRFLOW DAG - PIPELINE COMPLET REDDIT CAN 2025
========================================================
Orchestration du pipeline Big Data de bout en bout:
1. V√©rification infrastructure
2. Scraping Reddit ‚Üí Kafka (10 min)
3. Attente traitement Spark Streaming
4. Analyse ML Sentiment
5. Validation et rapport
========================================================
"""

from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.docker.operators.docker import DockerOperator
from airflow.utils.dates import days_ago
from datetime import datetime, timedelta
from pymongo import MongoClient
import logging

# ========================================
# CONFIGURATION
# ========================================
default_args = {
    'owner': 'bigdata_team',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=3),
}

MONGODB_URI = 'mongodb://admin:admin123@mongodb:27017/'
MIN_POSTS_FOR_ML = 50  # Seuil minimum avant ML

# ========================================
# FONCTIONS PYTHON
# ========================================

def check_infrastructure(**context):
    """V√©rifie que l'infrastructure est op√©rationnelle"""
    logging.info("üîç V√©rification de l'infrastructure...")
    
    try:
        # V√©rifier MongoDB
        client = MongoClient(MONGODB_URI, serverSelectionTimeoutMS=5000)
        client.admin.command('ping')
        logging.info("‚úÖ MongoDB op√©rationnel")
        client.close()
        
        return True
    except Exception as e:
        logging.error(f"‚ùå Infrastructure error: {e}")
        raise

def get_pipeline_stats(**context):
    """R√©cup√®re les stats du pipeline"""
    logging.info("üìä R√©cup√©ration des statistiques...")
    
    try:
        client = MongoClient(MONGODB_URI)
        db = client['reddit_can']
        
        stats = {
            'timestamp': datetime.now().isoformat(),
            'posts': db['posts'].count_documents({}),
            'comments': db['comments'].count_documents({}),
            'processed_posts': db['processed_posts'].count_documents({}),
            'sentiment_results': db['sentiment_results'].count_documents({})
        }
        
        logging.info(f"üìà Stats: {stats}")
        
        client.close()
        
        # Pousser vers XCom pour les autres t√¢ches
        context['task_instance'].xcom_push(key='pipeline_stats', value=stats)
        
        return stats
        
    except Exception as e:
        logging.error(f"‚ùå Erreur stats: {e}")
        raise

def check_ml_threshold(**context):
    """V√©rifie si on a assez de donn√©es pour le ML"""
    logging.info("üîç V√©rification du seuil ML...")
    
    try:
        # R√©cup√©rer les stats depuis XCom
        stats = context['task_instance'].xcom_pull(
            task_ids='stats_after_scraping',
            key='pipeline_stats'
        )
        
        if not stats:
            logging.warning("‚ö†Ô∏è Pas de stats disponibles")
            return 'skip_ml'
        
        processed_count = stats.get('processed_posts', 0)
        
        logging.info(f"üìä Posts trait√©s: {processed_count}")
        logging.info(f"üéØ Seuil minimum: {MIN_POSTS_FOR_ML}")
        
        if processed_count >= MIN_POSTS_FOR_ML:
            logging.info("‚úÖ Seuil atteint ‚Üí Lancement du ML")
            return 'run_ml_analysis'
        else:
            logging.warning(f"‚ö†Ô∏è Seuil non atteint ({processed_count}/{MIN_POSTS_FOR_ML})")
            return 'skip_ml'
            
    except Exception as e:
        logging.error(f"‚ùå Erreur v√©rification: {e}")
        return 'skip_ml'

def generate_final_report(**context):
    """G√©n√®re un rapport complet du pipeline"""
    logging.info("\n" + "="*70)
    logging.info("üìä RAPPORT FINAL DU PIPELINE")
    logging.info("="*70)
    
    try:
        # R√©cup√©rer les stats finales
        stats = context['task_instance'].xcom_pull(
            task_ids='stats_final',
            key='pipeline_stats'
        )
        
        if not stats:
            logging.warning("‚ö†Ô∏è Pas de stats finales")
            return
        
        # Rapport d√©taill√©
        logging.info(f"\nüìÖ Timestamp: {stats['timestamp']}")
        logging.info(f"\nüìä Volume de donn√©es:")
        logging.info(f"   Posts bruts:         {stats['posts']:>5}")
        logging.info(f"   Commentaires:        {stats['comments']:>5}")
        logging.info(f"   Posts trait√©s:       {stats['processed_posts']:>5}")
        logging.info(f"   Sentiments analys√©s: {stats['sentiment_results']:>5}")
        
        # Taux de couverture
        if stats['processed_posts'] > 0:
            coverage = (stats['sentiment_results'] / stats['processed_posts']) * 100
            logging.info(f"\n‚úÖ Taux de couverture ML: {coverage:.1f}%")
        
        # Distribution des sentiments
        client = MongoClient(MONGODB_URI)
        db = client['reddit_can']
        
        pipeline = [
            {'$group': {
                '_id': '$predicted_sentiment',
                'count': {'$sum': 1}
            }}
        ]
        
        sentiment_dist = list(db['sentiment_results'].aggregate(pipeline))
        
        if sentiment_dist:
            logging.info(f"\nüí≠ Distribution des sentiments:")
            for item in sentiment_dist:
                logging.info(f"   {item['_id']:15s}: {item['count']:5d}")
        
        client.close()
        
        # Recommandations
        logging.info(f"\nüí° Recommandations:")
        if stats['posts'] < 100:
            logging.warning("   ‚ö†Ô∏è Volume de posts faible - √âlargir les crit√®res de scraping")
        if stats['sentiment_results'] < 50:
            logging.warning("   ‚ö†Ô∏è Peu de sentiments analys√©s - Augmenter la fr√©quence")
        if stats['posts'] >= 300:
            logging.info("   ‚úÖ Volume de donn√©es excellent")
        
        logging.info("="*70)
        
        # Sauvegarder le rapport
        context['task_instance'].xcom_push(key='final_report', value=stats)
        
        return stats
        
    except Exception as e:
        logging.error(f"‚ùå Erreur g√©n√©ration rapport: {e}")
        raise

def cleanup_old_data(**context):
    """Nettoie les anciennes donn√©es (optionnel)"""
    logging.info("üßπ Nettoyage des donn√©es anciennes...")
    
    try:
        client = MongoClient(MONGODB_URI)
        db = client['reddit_can']
        
        # Garder seulement les 7 derniers jours
        cutoff_date = datetime.now() - timedelta(days=7)
        
        # Compter avant
        old_posts = db['posts'].count_documents({
            'created_date': {'$lt': cutoff_date.isoformat()}
        })
        
        if old_posts > 0:
            logging.info(f"üóëÔ∏è Suppression de {old_posts} anciens posts")
            # D√©commenter pour activer le nettoyage
            # db['posts'].delete_many({'created_date': {'$lt': cutoff_date.isoformat()}})
        else:
            logging.info("‚úÖ Pas de donn√©es anciennes √† supprimer")
        
        client.close()
        
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Erreur nettoyage: {e}")

# ========================================
# DAG DEFINITION
# ========================================
with DAG(
    dag_id='reddit_can_complete_pipeline',
    default_args=default_args,
    description='Pipeline Big Data complet - Reddit CAN 2025',
    schedule_interval='0 */6 * * *',  # Toutes les 6 heures
    start_date=days_ago(1),
    catchup=False,
    tags=['bigdata', 'reddit', 'can2025', 'production'],
    doc_md="""
    # Pipeline Reddit CAN 2025 - Production
    
    ## Objectif
    Collecte, traitement et analyse de sentiments des discussions Reddit 
    sur la CAN 2025 (Coupe d'Afrique des Nations).
    
    ## Flux
    1. V√©rification infrastructure (Kafka, MongoDB)
    2. Scraping Reddit ‚Üí Kafka (10 minutes)
    3. Attente traitement Spark Streaming (60s)
    4. Analyse ML Sentiment (si >= 50 posts)
    5. G√©n√©ration rapport final
    6. Nettoyage optionnel
    
    ## Fr√©quence
    Toutes les 6 heures
    
    ## Technologies
    - Kafka: Streaming temps r√©el
    - Spark: Traitement massif parall√®le
    - MongoDB: Stockage NoSQL
    - Spark ML: Analyse de sentiments (84% accuracy)
    - Streamlit: Dashboard interactif
    """
) as dag:

    # ========================================
    # T√ÇCHE 1: V√©rification Infrastructure
    # ========================================
    check_infra = PythonOperator(
        task_id='check_infrastructure',
        python_callable=check_infrastructure,
        doc_md="V√©rifie que MongoDB et Kafka sont op√©rationnels"
    )

    # ========================================
    # T√ÇCHE 2: Stats AVANT scraping
    # ========================================
    stats_before = PythonOperator(
        task_id='stats_before_scraping',
        python_callable=get_pipeline_stats,
        doc_md="R√©cup√®re les statistiques avant le scraping"
    )

    # ========================================
    # T√ÇCHE 3: SCRAPING Reddit ‚Üí Kafka
    # ========================================
    scraping = DockerOperator(
        task_id='scraping_reddit_to_kafka',
        image='projet-bigdata-can-scraper-producer:latest',
        container_name='airflow_scraper_{{ ts_nodash }}',
        api_version='auto',
        auto_remove=True,
        docker_url='unix://var/run/docker.sock',
        network_mode='projet-bigdata-can_bigdata-network',
        environment={
            'KAFKA_BOOTSTRAP_SERVERS': 'kafka:29092',
            'KAFKA_TOPIC': 'reddit-can-posts',
            'SCRAPING_INTERVAL': '600',  # 10 minutes
            'PYTHONUNBUFFERED': '1'
        },
        execution_timeout=timedelta(minutes=12),
        mount_tmp_dir=False,
        doc_md="Lance le scraper Reddit pour 10 minutes de collecte intensive"
    )

    # ========================================
    # T√ÇCHE 4: Attendre Spark Streaming
    # ========================================
    wait_processing = BashOperator(
        task_id='wait_spark_streaming',
        bash_command='echo "‚è≥ Attente traitement Spark Streaming (60s)..." && sleep 60',
        doc_md="Attend que Spark Streaming traite les donn√©es"
    )

    # ========================================
    # T√ÇCHE 5: Stats APR√àS scraping
    # ========================================
    stats_after = PythonOperator(
        task_id='stats_after_scraping',
        python_callable=get_pipeline_stats,
        doc_md="R√©cup√®re les statistiques apr√®s le scraping"
    )

    # ========================================
    # T√ÇCHE 6: V√©rifier seuil ML
    # ========================================
    check_threshold = BranchPythonOperator(
        task_id='check_ml_threshold',
        python_callable=check_ml_threshold,
        doc_md="V√©rifie si on a assez de donn√©es (>= 50 posts) pour lancer le ML"
    )

    # ========================================
    # T√ÇCHE 7a: LANCER ML
    # ========================================
    run_ml = DockerOperator(
        task_id='run_ml_analysis',
        image='projet-bigdata-can-spark-ml-sentiment:latest',
        container_name='airflow_spark_ml_{{ ts_nodash }}',
        api_version='auto',
        auto_remove=True,
        docker_url='unix://var/run/docker.sock',
        network_mode='projet-bigdata-can_bigdata-network',
        environment={
            'MONGODB_URI': 'mongodb://admin:admin123@mongodb:27017/reddit_can?authSource=admin'
        },
        execution_timeout=timedelta(minutes=15),
        mount_tmp_dir=False,
        trigger_rule='none_failed_min_one_success',
        doc_md="Lance l'analyse de sentiments avec Spark ML (Random Forest 84% accuracy)"
    )

    # ========================================
    # T√ÇCHE 7b: SKIP ML
    # ========================================
    skip_ml = BashOperator(
        task_id='skip_ml',
        bash_command='echo "‚ö†Ô∏è ML skipp√© - Pas assez de donn√©es (< 50 posts)"',
        trigger_rule='none_failed_min_one_success',
        doc_md="Skip si pas assez de donn√©es"
    )

    # ========================================
    # T√ÇCHE 8: Stats FINALES
    # ========================================
    stats_final = PythonOperator(
        task_id='stats_final',
        python_callable=get_pipeline_stats,
        trigger_rule='none_failed_min_one_success',
        doc_md="R√©cup√®re les statistiques finales apr√®s ML"
    )

    # ========================================
    # T√ÇCHE 9: G√©n√©ration Rapport
    # ========================================
    generate_report = PythonOperator(
        task_id='generate_final_report',
        python_callable=generate_final_report,
        trigger_rule='none_failed_min_one_success',
        doc_md="G√©n√®re un rapport complet avec toutes les m√©triques"
    )

    # ========================================
    # T√ÇCHE 10: Nettoyage (optionnel)
    # ========================================
    cleanup = PythonOperator(
        task_id='cleanup_old_data',
        python_callable=cleanup_old_data,
        trigger_rule='all_done',
        doc_md="Nettoie les donn√©es de plus de 7 jours (optionnel)"
    )

    # ========================================
    # T√ÇCHE 11: Notification Finale
    # ========================================
    notify = BashOperator(
        task_id='pipeline_success',
        bash_command="""
        echo "=========================================="
        echo "‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS"
        echo "=========================================="
        echo ""
        echo "üåê Dashboard: http://localhost:8501"
        echo "üìä MongoDB: http://localhost:8081"
        echo ""
        echo "üìù Logs complets dans Airflow UI"
        echo "=========================================="
        """,
        trigger_rule='all_done',
        doc_md="Affiche les informations finales"
    )

    # ========================================
    # D√âFINITION DU FLUX
    # ========================================
    
    # Phase 1: Infrastructure et collecte
    check_infra >> stats_before >> scraping >> wait_processing >> stats_after
    
    # Phase 2: D√©cision ML
    stats_after >> check_threshold
    check_threshold >> [run_ml, skip_ml]
    
    # Phase 3: Finalisation
    [run_ml, skip_ml] >> stats_final >> generate_report >> cleanup >> notify